#!/usr/bin/env python3
from __future__ import annotations
"""
è®°å¿†ç®¡ç†æ¨¡å— â€” é˜²é€‰é¢˜æ’è½¦ã€é˜²è§‚ç‚¹çŸ›ç›¾ã€è¿½è¸ªåˆ›ä½œå†å²ã€‚

åŠŸèƒ½:
  1. è®°å½•æ¯æ¬¡åˆ›ä½œï¼ˆé€‰é¢˜ã€ç«‹åœºã€å¼•ç”¨æ•°æ®ï¼‰
  2. æ£€æŸ¥æ–°é€‰é¢˜æ˜¯å¦ä¸å†å²å†²çª
  3. æ£€æŸ¥ç«‹åœºæ˜¯å¦ä¸ä¹‹å‰çŸ›ç›¾
  4. æŸ¥è¯¢å’Œæœç´¢å†å²è®°å½•

å­˜å‚¨æ ¼å¼:
    memory/YYYY-MM-DD.json â€” æ¯å¤©ä¸€ä¸ªæ–‡ä»¶ï¼Œè¿½åŠ å†™å…¥
    æ¯æ¡è®°å½•: {
        "timestamp": "2026-02-12T21:30:00",
        "topic": "é€‰é¢˜æ ‡é¢˜",
        "stance": {"AI Agent": "çœ‹å¤š", "Tesla FSD": "è°¨æ…ä¹è§‚"},
        "data_cited": ["GPT-5 å‚æ•° 10T", "æ¨ç†æˆæœ¬é™ 80%"],
        "platforms": ["é£ä¹¦", "çŸ¥ä¹"],
        "status": "draft|published|abandoned"
    }

ç”¨æ³•:
    # è®°å½•ä¸€æ¬¡åˆ›ä½œ
    python memory.py log --topic "GPT-5 å‘å¸ƒ" --stance "AI Agent:çœ‹å¤š" --data-cited "å‚æ•°10T, æˆæœ¬é™80%"

    # æ£€æŸ¥é€‰é¢˜æ˜¯å¦å†™è¿‡
    python memory.py check --topic "GPT-5 ç›¸å…³"

    # æ£€æŸ¥ç«‹åœºæ˜¯å¦çŸ›ç›¾
    python memory.py stance --entity "Tesla FSD" --new-stance "çœ‹ç©º"

    # æœç´¢å†å²
    python memory.py search --query "GPT"

    # åˆ—å‡ºæœ€è¿‘è®°å½•
    python memory.py list --days 7
"""

import argparse
import json
import os
import sys
from datetime import datetime, timedelta
from typing import Any
from pathlib import Path

SKILL_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MEMORY_DIR = os.path.join(SKILL_DIR, "memory")


def ensure_memory_dir():
    """ç¡®ä¿è®°å¿†ç›®å½•å­˜åœ¨"""
    os.makedirs(MEMORY_DIR, exist_ok=True)


def get_today_file() -> str:
    """è·å–ä»Šå¤©çš„è®°å¿†æ–‡ä»¶è·¯å¾„"""
    return os.path.join(MEMORY_DIR, f"{datetime.now().strftime('%Y-%m-%d')}.json")


def load_day_entries(filepath: str) -> List[Dict]:
    """åŠ è½½æŸå¤©çš„è®°å¿†æ¡ç›®"""
    if not os.path.exists(filepath):
        return []
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return []


def save_day_entries(filepath: str, entries: List[Dict]):
    """ä¿å­˜æŸå¤©çš„è®°å¿†æ¡ç›®"""
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(entries, f, ensure_ascii=False, indent=2)


def load_all_entries(days: int = 30) -> List[Dict]:
    """åŠ è½½æœ€è¿‘ N å¤©çš„æ‰€æœ‰è®°å¿†æ¡ç›®"""
    ensure_memory_dir()
    all_entries = []
    
    for i in range(days):
        date = datetime.now() - timedelta(days=i)
        filepath = os.path.join(MEMORY_DIR, f"{date.strftime('%Y-%m-%d')}.json")
        entries = load_day_entries(filepath)
        for entry in entries:
            entry["_date"] = date.strftime("%Y-%m-%d")
        all_entries.extend(entries)
    
    return all_entries


def _similarity(a: str, b: str) -> float:
    """å­—çº§ Jaccard ç›¸ä¼¼åº¦"""
    set_a = set(a)
    set_b = set(b)
    if not set_a or not set_b:
        return 0.0
    return len(set_a & set_b) / len(set_a | set_b)


def _keyword_overlap(a: str, b: str) -> float:
    """å…³é”®è¯é‡å åº¦"""
    # åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    import re
    words_a = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', a.lower()))
    words_b = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', b.lower()))
    
    if not words_a or not words_b:
        return 0.0
    
    return len(words_a & words_b) / min(len(words_a), len(words_b))


# ------ å‘½ä»¤å®ç° ------

def cmd_log(args):
    """è®°å½•ä¸€æ¬¡åˆ›ä½œ"""
    ensure_memory_dir()
    
    # è§£æç«‹åœº
    stance = {}
    if args.stance:
        for item in args.stance.split(","):
            item = item.strip()
            if ":" in item:
                entity, opinion = item.split(":", 1)
                stance[entity.strip()] = opinion.strip()
            elif "ï¼š" in item:
                entity, opinion = item.split("ï¼š", 1)
                stance[entity.strip()] = opinion.strip()
            else:
                stance[item] = "ä¸­æ€§"
    
    # è§£æå¼•ç”¨æ•°æ®
    data_cited = []
    if args.data_cited:
        data_cited = [d.strip() for d in args.data_cited.split(",") if d.strip()]
    
    # è§£æå¹³å°
    platforms = []
    if hasattr(args, "platforms") and args.platforms:
        platforms = [p.strip() for p in args.platforms.split(",") if p.strip()]
    
    entry = {
        "timestamp": datetime.now().isoformat(),
        "topic": args.topic,
        "stance": stance,
        "data_cited": data_cited,
        "platforms": platforms,
        "status": getattr(args, "status", "draft"),
        "notes": getattr(args, "notes", ""),
    }
    
    # è¿½åŠ åˆ°ä»Šå¤©çš„æ–‡ä»¶
    filepath = get_today_file()
    entries = load_day_entries(filepath)
    entries.append(entry)
    save_day_entries(filepath, entries)
    
    print(f"âœ… è®°å¿†å·²è®°å½•")
    print(f"   é€‰é¢˜: {args.topic}")
    if stance:
        print(f"   ç«‹åœº: {stance}")
    if data_cited:
        print(f"   æ•°æ®: {data_cited}")
    print(f"   æ–‡ä»¶: {filepath}")


def cmd_check(args):
    """æ£€æŸ¥é€‰é¢˜æ˜¯å¦å†™è¿‡ï¼ˆé˜²æ’è½¦ï¼‰"""
    all_entries = load_all_entries(days=90)
    
    if not all_entries:
        print("âœ… è®°å¿†åº“ä¸ºç©ºï¼Œæ²¡æœ‰å†²çªã€‚")
        return
    
    topic = args.topic
    collisions = []
    
    for entry in all_entries:
        entry_topic = entry.get("topic", "")
        sim_char = _similarity(topic, entry_topic)
        sim_kw = _keyword_overlap(topic, entry_topic)
        
        # ç»¼åˆç›¸ä¼¼åº¦
        sim = max(sim_char, sim_kw)
        
        if sim > 0.4:
            collisions.append({
                "date": entry.get("_date", entry.get("timestamp", "")[:10]),
                "topic": entry_topic,
                "similarity": round(sim, 2),
                "stance": entry.get("stance", {}),
                "status": entry.get("status", "unknown"),
            })
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    collisions.sort(key=lambda x: x["similarity"], reverse=True)
    
    if not collisions:
        print(f"âœ… ã€Œ{topic}ã€æ²¡æœ‰å‘ç°å†²çªé€‰é¢˜ã€‚å¯ä»¥æ”¾å¿ƒå†™ï¼")
    else:
        print(f"âš ï¸ ã€Œ{topic}ã€å‘ç° {len(collisions)} ä¸ªç›¸ä¼¼é€‰é¢˜:")
        print()
        for c in collisions[:5]:
            emoji = "ğŸ”´" if c["similarity"] > 0.7 else "ğŸŸ¡" if c["similarity"] > 0.5 else "ğŸŸ¢"
            print(f"  {emoji} [{c['date']}] {c['topic']}")
            print(f"     ç›¸ä¼¼åº¦: {c['similarity']} | çŠ¶æ€: {c['status']}")
            if c["stance"]:
                print(f"     ç«‹åœº: {c['stance']}")
            print()
        
        if collisions[0]["similarity"] > 0.7:
            print("â›” å»ºè®®: é«˜åº¦ç›¸ä¼¼ï¼æ¢ä¸€ä¸ªè§’åº¦ï¼Œæˆ–è€…æ”¾å¼ƒè¿™ä¸ªé€‰é¢˜ã€‚")
        elif collisions[0]["similarity"] > 0.5:
            print("ğŸ’¡ å»ºè®®: æœ‰ä¸€å®šé‡å ï¼Œå»ºè®®æ¢ä¸€ä¸ªåˆ‡å…¥è§’åº¦ã€‚")
        else:
            print("âœ… ç›¸ä¼¼åº¦è¾ƒä½ï¼Œå¯ä»¥å†™ä½†æ³¨æ„å·®å¼‚åŒ–ã€‚")


def cmd_stance(args):
    """æ£€æŸ¥ç«‹åœºæ˜¯å¦çŸ›ç›¾"""
    all_entries = load_all_entries(days=90)
    
    entity = args.entity
    new_stance = args.new_stance
    
    # æœç´¢å†å²ç«‹åœº
    history = []
    for entry in all_entries:
        stances = entry.get("stance", {})
        for ent, opinion in stances.items():
            if _keyword_overlap(entity, ent) > 0.5:
                history.append({
                    "date": entry.get("_date", entry.get("timestamp", "")[:10]),
                    "entity": ent,
                    "opinion": opinion,
                    "topic": entry.get("topic", ""),
                })
    
    if not history:
        print(f"âœ… æ²¡æœ‰å…³äºã€Œ{entity}ã€çš„å†å²ç«‹åœºè®°å½•ã€‚å¯ä»¥è‡ªç”±è¡¨æ€ã€‚")
        return
    
    print(f"ğŸ“‹ å…³äºã€Œ{entity}ã€çš„å†å²ç«‹åœº:")
    print()
    for h in history:
        print(f"  [{h['date']}] {h['entity']}: {h['opinion']}")
        print(f"     æ–‡ç« : {h['topic']}")
    
    print()
    
    # æ£€æŸ¥çŸ›ç›¾
    latest = history[-1]
    if latest["opinion"] != new_stance:
        # ç®€å•åˆ¤æ–­æ˜¯å¦çŸ›ç›¾ï¼ˆçœ‹å¤š vs çœ‹ç©ºï¼‰
        opposites = {
            ("çœ‹å¤š", "çœ‹ç©º"), ("çœ‹ç©º", "çœ‹å¤š"),
            ("æ”¯æŒ", "åå¯¹"), ("åå¯¹", "æ”¯æŒ"),
            ("æ¨è", "ä¸æ¨è"), ("ä¸æ¨è", "æ¨è"),
            ("ä¹è§‚", "æ‚²è§‚"), ("æ‚²è§‚", "ä¹è§‚"),
        }
        
        pair = (latest["opinion"], new_stance)
        if pair in opposites:
            print(f"âš ï¸ ç«‹åœºåè½¬! ä¸Šæ¬¡({latest['date']}): {latest['opinion']} â†’ è¿™æ¬¡: {new_stance}")
            print(f"   å¦‚æœç¡®å®è¦æ”¹ï¼Œè¯·åœ¨æ–‡ç« ä¸­è¯´æ˜åŸå› ï¼ˆæ–°æ•°æ®ã€æ–°äº‹ä»¶ï¼‰ã€‚")
            print(f"   å¦åˆ™è¯»è€…ä¼šè§‰å¾—ä½ å‰åçŸ›ç›¾ã€‚")
        else:
            print(f"ğŸ’¡ ç«‹åœºæœ‰å˜åŒ–ä½†ä¸ç®—çŸ›ç›¾: {latest['opinion']} â†’ {new_stance}")
    else:
        print(f"âœ… ç«‹åœºä¸€è‡´: {new_stance}ã€‚æ²¡æœ‰çŸ›ç›¾ã€‚")


def cmd_search(args):
    """æœç´¢å†å²è®°å½•"""
    all_entries = load_all_entries(days=90)
    query = args.query.lower()
    
    results = []
    for entry in all_entries:
        # åœ¨é€‰é¢˜ã€ç«‹åœºã€æ•°æ®ä¸­æœç´¢
        searchable = json.dumps(entry, ensure_ascii=False).lower()
        if query in searchable:
            results.append(entry)
    
    if not results:
        print(f"ğŸ” æ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{args.query}ã€ç›¸å…³çš„è®°å½•ã€‚")
        return
    
    print(f"ğŸ” æ‰¾åˆ° {len(results)} æ¡ä¸ã€Œ{args.query}ã€ç›¸å…³çš„è®°å½•:")
    print()
    for entry in results[:10]:
        date = entry.get("_date", entry.get("timestamp", "")[:10])
        topic = entry.get("topic", "æœªçŸ¥")
        status = entry.get("status", "unknown")
        print(f"  [{date}] {topic} ({status})")
        if entry.get("stance"):
            print(f"    ç«‹åœº: {entry['stance']}")
        if entry.get("data_cited"):
            print(f"    æ•°æ®: {', '.join(entry['data_cited'][:3])}")
        print()


def cmd_list(args):
    """åˆ—å‡ºæœ€è¿‘çš„è®°å½•"""
    days = getattr(args, "days", 7)
    all_entries = load_all_entries(days=days)
    
    if not all_entries:
        print(f"ğŸ“‹ æœ€è¿‘ {days} å¤©å†…æ²¡æœ‰åˆ›ä½œè®°å½•ã€‚")
        return
    
    print(f"ğŸ“‹ æœ€è¿‘ {days} å¤©çš„åˆ›ä½œè®°å½• (å…± {len(all_entries)} æ¡):")
    print()
    
    for entry in all_entries:
        date = entry.get("_date", entry.get("timestamp", "")[:10])
        topic = entry.get("topic", "æœªçŸ¥")
        status = entry.get("status", "unknown")
        
        status_emoji = {"draft": "ğŸ“", "published": "âœ…", "abandoned": "âŒ"}.get(status, "â“")
        
        print(f"  {status_emoji} [{date}] {topic}")
        if entry.get("stance"):
            print(f"     ç«‹åœº: {entry['stance']}")
        if entry.get("platforms"):
            print(f"     å¹³å°: {', '.join(entry['platforms'])}")
    
    print()
    
    # ç»Ÿè®¡
    status_counts = {}
    for entry in all_entries:
        s = entry.get("status", "unknown")
        status_counts[s] = status_counts.get(s, 0) + 1
    
    print(f"ğŸ“Š ç»Ÿè®¡: {status_counts}")


def main():
    parser = argparse.ArgumentParser(description="è®°å¿†ç®¡ç†æ¨¡å—")
    subparsers = parser.add_subparsers(dest="command", help="æ“ä½œ")
    
    # log
    log_parser = subparsers.add_parser("log", help="è®°å½•ä¸€æ¬¡åˆ›ä½œ")
    log_parser.add_argument("--topic", required=True, help="é€‰é¢˜æ ‡é¢˜")
    log_parser.add_argument("--stance", help="ç«‹åœºï¼ˆæ ¼å¼: å®ä½“:è§‚ç‚¹,å®ä½“:è§‚ç‚¹ï¼‰")
    log_parser.add_argument("--data-cited", help="å¼•ç”¨çš„æ•°æ®ï¼ˆé€—å·åˆ†éš”ï¼‰")
    log_parser.add_argument("--platforms", help="å‘å¸ƒå¹³å°ï¼ˆé€—å·åˆ†éš”ï¼‰")
    log_parser.add_argument("--status", default="draft", choices=["draft", "published", "abandoned"],
                           help="çŠ¶æ€")
    log_parser.add_argument("--notes", default="", help="å¤‡æ³¨")
    
    # check
    check_parser = subparsers.add_parser("check", help="æ£€æŸ¥é€‰é¢˜æ˜¯å¦å†™è¿‡")
    check_parser.add_argument("--topic", required=True, help="è¦æ£€æŸ¥çš„é€‰é¢˜æ ‡é¢˜")
    
    # stance
    stance_parser = subparsers.add_parser("stance", help="æ£€æŸ¥ç«‹åœºæ˜¯å¦çŸ›ç›¾")
    stance_parser.add_argument("--entity", required=True, help="å®ä½“åç§°")
    stance_parser.add_argument("--new-stance", required=True, help="æ–°ç«‹åœº")
    
    # search
    search_parser = subparsers.add_parser("search", help="æœç´¢å†å²è®°å½•")
    search_parser.add_argument("--query", required=True, help="æœç´¢å…³é”®è¯")
    
    # list
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºæœ€è¿‘è®°å½•")
    list_parser.add_argument("--days", type=int, default=7, help="æŸ¥çœ‹æœ€è¿‘å‡ å¤©")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    ensure_memory_dir()
    
    dispatch = {
        "log": cmd_log,
        "check": cmd_check,
        "stance": cmd_stance,
        "search": cmd_search,
        "list": cmd_list,
    }
    
    dispatch[args.command](args)


if __name__ == "__main__":
    main()
