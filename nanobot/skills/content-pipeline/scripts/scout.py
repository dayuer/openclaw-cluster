#!/usr/bin/env python3
from __future__ import annotations
"""
çƒ­ç‚¹ä¾¦å¯Ÿ + é€‰é¢˜ç­›é€‰

åŠŸèƒ½:
  1. é€šè¿‡ nanobot å†…ç½®çš„ web_search æœç´¢çƒ­ç‚¹ï¼ˆæœ¬è„šæœ¬ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼‰
  2. å¯¹æœç´¢ç»“æœè¿›è¡Œç­›é€‰ã€è¯„åˆ†ã€æ’åº
  3. è¾“å‡ºæ ¼å¼åŒ–çš„é€‰é¢˜åˆ—è¡¨

ç”¨æ³•:
    # ç”ŸæˆæŸ¥è¯¢å»ºè®® + ç­›é€‰ç»“æœ
    python scout.py --keywords "AI Agent,SaaS" --count 5

    # ä»å·²æœ‰ç´ ææ–‡ä»¶ç­›é€‰ï¼ˆnanobot æœç´¢åä¿å­˜çš„ç»“æœï¼‰
    python scout.py --from-file /tmp/nanobot/search_results.md --count 5

    # è¾“å‡ºåˆ°æ–‡ä»¶
    python scout.py --keywords "AI,åˆ›ä¸š" --output /tmp/nanobot/topics.md

è®¾è®¡æ€è·¯:
    æœ¬è„šæœ¬ **ä¸ç›´æ¥è°ƒç”¨æœç´¢ API**ï¼ˆé‚£æ˜¯ nanobot V3 çš„æ´»ï¼‰ã€‚
    å®ƒè´Ÿè´£ï¼š
    1. ç”Ÿæˆæœ€ä¼˜æœç´¢æŸ¥è¯¢ç»„åˆ
    2. å¦‚æœæœ‰ç´ ææ–‡ä»¶ï¼Œå¯¹å…¶è¿›è¡Œç­›é€‰å’Œè¯„åˆ†
    3. è¾“å‡ºæ ¼å¼åŒ–çš„é€‰é¢˜å»ºè®®

    å…¸å‹è°ƒç”¨é“¾ï¼š
    V3: web_search("AI Agent framework trending")
    V3: write_file /tmp/nanobot/search_results.md "æœç´¢ç»“æœ..."
    V3: exec python scout.py --from-file /tmp/nanobot/search_results.md
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime
from dataclasses import dataclass, field, asdict

SKILL_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MEMORY_DIR = os.path.join(SKILL_DIR, "memory")


@dataclass
class TopicCandidate:
    """é€‰é¢˜å€™é€‰"""
    title: str
    source: str = ""
    url: str = ""
    hot_score: float = 0.0
    relevance_score: float = 0.0
    controversy_score: float = 0.0
    angles: List[str] = field(default_factory=list)
    key_points: List[str] = field(default_factory=list)
    one_liner: str = ""


# ------ æŸ¥è¯¢ç”Ÿæˆå™¨ ------

QUERY_TEMPLATES = [
    # (æ¨¡æ¿, æè¿°)
    ("{keyword} æœ€æ–°äº‰è®® OR é‡å¤§æ›´æ–° site:twitter.com OR site:reddit.com", "ç¤¾äº¤çƒ­è®®"),
    ("{keyword} github trending 2026", "æŠ€æœ¯è¶‹åŠ¿"),
    ("{keyword} pricing model change OR å®šä»·è°ƒæ•´", "å•†ä¸šå˜åŠ¨"),
    ("{keyword} èèµ„ OR raised OR funding 2026", "èèµ„åŠ¨æ€"),
    ("{keyword} å®æµ‹ OR review OR è¯„æµ‹ 2026", "æ·±åº¦è¯„æµ‹"),
    ("{keyword} vs å¯¹æ¯” 2026", "äº§å“å¯¹æ¯”"),
    ("{keyword} ç”šè‡³ OR å±…ç„¶ OR æ²¡æƒ³åˆ°", "åç›´è§‰å†…å®¹"),
]


def generate_queries(keywords: str) -> List[dict]:
    """ç”Ÿæˆæœ€ä¼˜æœç´¢æŸ¥è¯¢ç»„åˆ"""
    keyword_list = [k.strip() for k in keywords.split(",") if k.strip()]
    queries = []

    for kw in keyword_list:
        for template, desc in QUERY_TEMPLATES[:3]:  # æ¯ä¸ªå…³é”®è¯å–å‰3ä¸ªæ¨¡æ¿
            query = template.format(keyword=kw)
            queries.append({
                "query": query,
                "keyword": kw,
                "type": desc,
            })

    return queries


# ------ ç´ æç­›é€‰å™¨ ------

# æ–°é—»é€šç¨¿ç‰¹å¾è¯ï¼ˆç”¨äºè¿‡æ»¤ä½è´¨é‡å†…å®¹ï¼‰
PRESS_RELEASE_SIGNALS = [
    "æ–°é—»ç¨¿", "å‘å¸ƒä¼š", "å®£å¸ƒ", "éš†é‡æ¨å‡º", "æ­£å¼å‘å¸ƒ",
    "è¡Œä¸šé¢†å…ˆ", "å…¨çƒé¢†å…ˆ", "å¼€åˆ›æ€§", "é‡Œç¨‹ç¢‘å¼",
    "æˆ˜ç•¥åˆä½œ", "å…¨é¢åˆä½œ", "è¾¾æˆåˆä½œ",
]

# è§‚ç‚¹/äº‰è®®ä¿¡å·è¯ï¼ˆæœ‰ä»·å€¼å†…å®¹çš„æ ‡å¿—ï¼‰
OPINION_SIGNALS = [
    "ä½†æ˜¯", "ç„¶è€Œ", "ä¸è¿‡", "äº‰è®®", "è´¨ç–‘", "é—®é¢˜æ˜¯",
    "å…¶å®", "åè€Œ", "æ²¡æƒ³åˆ°", "ç”šè‡³", "ç«Ÿç„¶",
    "å®æµ‹", "è¸©å‘", "çœŸå®ä½“éªŒ", "ç»éªŒ", "æ•™è®­",
    "æ•°æ®", "å¯¹æ¯”", "vs", "æµ‹è¯•ç»“æœ",
]

# ç”¨æˆ·å®šä½å…³é”®è¯ï¼ˆå½±å“ç›¸å…³æ€§è¯„åˆ†ï¼‰
USER_PROFILE_KEYWORDS = [
    "AI", "Agent", "LLM", "ç¼–ç¨‹", "coding", "programming",
    "å‡ºæµ·", "æµ·å¤–", "SaaS", "ç‹¬ç«‹å¼€å‘", "åˆ›ä¸š", "startup",
    "æ•ˆç‡", "è‡ªåŠ¨åŒ–", "å·¥å…·", "äº§å“",
]


def score_content(text: str, title: str) -> dict:
    """å¯¹ä¸€æ¡å†…å®¹è¿›è¡Œè¯„åˆ†"""
    text_lower = text.lower()
    title_lower = title.lower()

    # 1. é€šç¨¿è¿‡æ»¤
    press_count = sum(1 for signal in PRESS_RELEASE_SIGNALS if signal in text)
    is_press_release = press_count >= 3

    # 2. è§‚ç‚¹/äº‰è®®è¯„åˆ† (0-10)
    opinion_count = sum(1 for signal in OPINION_SIGNALS if signal in text)
    controversy_score = min(opinion_count * 1.5, 10.0)

    # 3. ç›¸å…³æ€§è¯„åˆ† (0-10)
    relevance_count = sum(1 for kw in USER_PROFILE_KEYWORDS if kw.lower() in text_lower or kw.lower() in title_lower)
    relevance_score = min(relevance_count * 1.5, 10.0)

    # 4. çƒ­åº¦è¯„åˆ† (åŸºäºå¯å‘å¼)
    hot_score = 5.0  # åŸºç¡€åˆ†
    # æœ‰æ•°å­— = å¯èƒ½æœ‰æ•°æ® = åŠ åˆ†
    if re.search(r'\d+%|\d+[ä¸‡äº¿]|$\d+|Â¥\d+', text):
        hot_score += 1.5
    # æœ‰ä»£ç  = æŠ€æœ¯æ·±åº¦ = åŠ åˆ†
    if "```" in text or "github.com" in text_lower:
        hot_score += 1.0
    # æœ‰åäºº/çŸ¥åå…¬å¸ = åŠ åˆ†
    known_entities = ["OpenAI", "Google", "Meta", "Tesla", "Elon", "Sam Altman", "å­—èŠ‚", "é˜¿é‡Œ", "è…¾è®¯"]
    entity_count = sum(1 for e in known_entities if e.lower() in text_lower)
    hot_score += min(entity_count * 0.8, 3.0)
    hot_score = min(hot_score, 10.0)

    return {
        "is_press_release": is_press_release,
        "hot_score": round(hot_score, 1),
        "relevance_score": round(relevance_score, 1),
        "controversy_score": round(controversy_score, 1),
        "composite_score": round(
            hot_score * 0.3 + relevance_score * 0.4 + controversy_score * 0.3,
            1
        ),
    }


def parse_search_results(content: str) -> List[TopicCandidate]:
    """ä»æœç´¢ç»“æœæ–‡ä»¶è§£æå‡ºé€‰é¢˜å€™é€‰"""
    candidates = []

    # å°è¯•æŒ‰æ®µè½åˆ†å‰²
    sections = re.split(r'\n(?=#{1,3}\s|---|\*\*)', content)

    for section in sections:
        section = section.strip()
        if not section or len(section) < 50:
            continue

        # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œæˆ– ## æ ‡é¢˜ï¼‰
        lines = section.split("\n")
        title = lines[0].strip().lstrip("#").strip().strip("*").strip()
        if not title or len(title) < 5:
            continue

        # æå– URL
        urls = re.findall(r'https?://[^\s\)]+', section)
        url = urls[0] if urls else ""

        # è¯„åˆ†
        scores = score_content(section, title)

        # è·³è¿‡é€šç¨¿
        if scores["is_press_release"]:
            continue

        # ç”Ÿæˆåˆ‡å…¥è§’åº¦
        angles = []
        if scores["controversy_score"] > 5:
            angles.append("äº‰è®®åˆ†æ")
        if scores["hot_score"] > 7:
            angles.append("çƒ­ç‚¹è§£è¯»")
        if "å®æµ‹" in section or "è¯„æµ‹" in section:
            angles.append("å®æ“è¯„æµ‹")
        if "vs" in section.lower() or "å¯¹æ¯”" in section:
            angles.append("äº§å“å¯¹æ¯”")
        if not angles:
            angles.append("æ·±åº¦åˆ†æ")

        # ç”Ÿæˆä¸€å¥è¯æ ¸å¿ƒçœ‹ç‚¹
        one_liner = title[:50]
        for signal in OPINION_SIGNALS:
            idx = section.find(signal)
            if idx > 0:
                sentence_end = section.find("ã€‚", idx)
                if sentence_end > 0:
                    one_liner = section[idx:sentence_end + 1][:80]
                    break

        candidate = TopicCandidate(
            title=title[:60],
            source="web_search",
            url=url,
            hot_score=scores["hot_score"],
            relevance_score=scores["relevance_score"],
            controversy_score=scores["controversy_score"],
            angles=angles,
            key_points=[],
            one_liner=one_liner,
        )
        candidates.append(candidate)

    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
    candidates.sort(
        key=lambda c: c.hot_score * 0.3 + c.relevance_score * 0.4 + c.controversy_score * 0.3,
        reverse=True,
    )

    return candidates


def check_memory_collision(topic_title: str) -> Optional[dict]:
    """æ£€æŸ¥é€‰é¢˜æ˜¯å¦ä¸è®°å¿†ä¸­çš„å†å²é¢˜ç›®å†²çª"""
    if not os.path.exists(MEMORY_DIR):
        return None

    for filename in os.listdir(MEMORY_DIR):
        if not filename.endswith(".json"):
            continue
        filepath = os.path.join(MEMORY_DIR, filename)
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                entries = json.load(f)
            for entry in entries:
                if entry.get("topic") and _similarity(topic_title, entry["topic"]) > 0.6:
                    return entry
        except (json.JSONDecodeError, KeyError):
            continue

    return None


def _similarity(a: str, b: str) -> float:
    """ç®€å•çš„å­—ç¬¦çº§ Jaccard ç›¸ä¼¼åº¦"""
    set_a = set(a)
    set_b = set(b)
    if not set_a or not set_b:
        return 0.0
    return len(set_a & set_b) / len(set_a | set_b)


# ------ è¾“å‡ºæ ¼å¼åŒ– ------

def format_topics_compact(candidates: List[TopicCandidate], count: int = 5) -> str:
    """ç´§å‡‘æ ¼å¼ï¼šç»™ IM æ¨é€ç”¨"""
    lines = [f"ğŸ“° ä»Šæ—¥é€‰é¢˜å»ºè®® ({datetime.now().strftime('%m-%d %H:%M')})"]
    lines.append("")

    for i, c in enumerate(candidates[:count], 1):
        composite = round(c.hot_score * 0.3 + c.relevance_score * 0.4 + c.controversy_score * 0.3, 1)
        lines.append(f"[{composite}] {i}. {c.title}")
        lines.append(f"   è§’åº¦: {' / '.join(c.angles)}")
        if c.one_liner and c.one_liner != c.title:
            lines.append(f"   çœ‹ç‚¹: {c.one_liner}")
        
        # æ£€æŸ¥è®°å¿†å†²çª
        collision = check_memory_collision(c.title)
        if collision:
            lines.append(f"   âš ï¸ æ³¨æ„: {collision.get('date', '')} å†™è¿‡ç±»ä¼¼é€‰é¢˜ \"{collision.get('topic', '')}\"")
        
        lines.append("")

    lines.append("ğŸ’¡ å›å¤æ•°å­—é€‰æ‹©è¦å†™çš„é€‰é¢˜ï¼ˆå¦‚ '1'ï¼‰")
    return "\n".join(lines)


def format_topics_full(candidates: List[TopicCandidate], count: int = 5) -> str:
    """å®Œæ•´æ ¼å¼ï¼šMarkdown æŠ¥å‘Š"""
    lines = [f"# ğŸ“° ä»Šæ—¥é€‰é¢˜å»ºè®®"]
    lines.append(f"\n> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append("")

    for i, c in enumerate(candidates[:count], 1):
        composite = round(c.hot_score * 0.3 + c.relevance_score * 0.4 + c.controversy_score * 0.3, 1)
        lines.append(f"## {i}. {c.title}")
        lines.append("")
        lines.append(f"- **ç»¼åˆè¯„åˆ†**: {composite}/10")
        lines.append(f"- **çƒ­åº¦**: {c.hot_score}/10 | **ç›¸å…³æ€§**: {c.relevance_score}/10 | **äº‰è®®æ€§**: {c.controversy_score}/10")
        lines.append(f"- **æ¥æº**: {c.source}")
        if c.url:
            lines.append(f"- **é“¾æ¥**: {c.url}")
        lines.append(f"- **åˆ‡å…¥è§’åº¦**: {' / '.join(c.angles)}")
        if c.one_liner and c.one_liner != c.title:
            lines.append(f"- **æ ¸å¿ƒçœ‹ç‚¹**: {c.one_liner}")
        
        collision = check_memory_collision(c.title)
        if collision:
            lines.append(f"- âš ï¸ **è®°å¿†å†²çª**: {collision.get('date', '')} å†™è¿‡ç±»ä¼¼é€‰é¢˜")
        
        lines.append("")
        lines.append("---")
        lines.append("")

    lines.append("\nğŸ’¡ å›å¤æ•°å­—é€‰æ‹©è¦å†™çš„é€‰é¢˜")
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="çƒ­ç‚¹ä¾¦å¯Ÿ + é€‰é¢˜ç­›é€‰")
    parser.add_argument("--keywords", default="AI Agent,SaaS,å‡ºæµ·,åˆ›ä¸š", help="æœç´¢å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰")
    parser.add_argument("--from-file", help="ä»æ–‡ä»¶è¯»å–æœç´¢ç»“æœï¼ˆè€Œéç”ŸæˆæŸ¥è¯¢ï¼‰")
    parser.add_argument("--count", type=int, default=5, help="è¾“å‡ºé€‰é¢˜æ•°é‡")
    parser.add_argument("--output", help="è¾“å‡ºåˆ°æ–‡ä»¶")
    parser.add_argument("--format", choices=["compact", "full"], default="full", help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("--queries-only", action="store_true", help="ä»…è¾“å‡ºæœç´¢æŸ¥è¯¢ï¼ˆä¸ç­›é€‰ï¼‰")

    args = parser.parse_args()

    if args.queries_only:
        # æ¨¡å¼ 1: åªç”ŸæˆæŸ¥è¯¢ï¼Œç»™ nanobot V3 å»æ‰§è¡Œæœç´¢
        queries = generate_queries(args.keywords)
        print("ğŸ” æ¨èæœç´¢æŸ¥è¯¢:")
        print()
        for i, q in enumerate(queries, 1):
            print(f"{i}. [{q['type']}] {q['query']}")
        
        print(f"\nå…± {len(queries)} æ¡æŸ¥è¯¢")
        print("V3 è¯·ä¾æ¬¡æ‰§è¡Œ web_searchï¼ŒæŠŠç»“æœåˆå¹¶ä¿å­˜åˆ°æ–‡ä»¶åï¼Œå†è°ƒç”¨ scout.py --from-file")
        return

    if args.from_file:
        # æ¨¡å¼ 2: ä»æ–‡ä»¶ç­›é€‰
        if not os.path.exists(args.from_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.from_file}", file=sys.stderr)
            sys.exit(1)
        
        with open(args.from_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        candidates = parse_search_results(content)
        print(f"âœ… ä»æ–‡ä»¶ä¸­ç­›é€‰å‡º {len(candidates)} ä¸ªé€‰é¢˜å€™é€‰")
    else:
        # æ¨¡å¼ 3: ç”ŸæˆæŸ¥è¯¢ + æç¤ºè°ƒç”¨æœç´¢
        queries = generate_queries(args.keywords)
        print("ğŸ“¡ çƒ­ç‚¹ä¾¦å¯Ÿ â€” æœç´¢æŸ¥è¯¢ç”Ÿæˆå®Œæˆ")
        print()
        print("æ¨èæœç´¢æŸ¥è¯¢ (V3 è¯·æ‰§è¡Œ web_search):")
        for i, q in enumerate(queries[:5], 1):
            print(f"  {i}. {q['query']}")
        print()
        print("æœç´¢å®Œæˆåï¼Œå°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶ï¼Œå†è°ƒç”¨:")
        print(f"  python scout.py --from-file /tmp/nanobot/search_results.md --count {args.count}")
        
        # åŒæ—¶è¾“å‡ºä¸€ä¸ªç©ºçš„æ¨¡æ¿æ–‡ä»¶
        candidates = []

    if candidates:
        if args.format == "compact":
            report = format_topics_compact(candidates, args.count)
        else:
            report = format_topics_full(candidates, args.count)
    else:
        report = "æš‚æ— ç­›é€‰ç»“æœã€‚è¯·å…ˆæ‰§è¡Œæœç´¢å¹¶ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ã€‚"

    if args.output:
        os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"\nğŸ“ é€‰é¢˜æŠ¥å‘Šå·²ä¿å­˜: {args.output}")
    else:
        print()
        print(report)


if __name__ == "__main__":
    main()
